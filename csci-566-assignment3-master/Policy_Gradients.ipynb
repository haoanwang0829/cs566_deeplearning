{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Policy Gradient Methods\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ayushj240/cs566-assignment3/blob/master/Policy_Gradients.ipynb)\n",
    "\n",
    "\n",
    "This notebook walks you through the essentials of Reinforcement Learning (RL) implementation. We hope after this you can comfortably train RL algorithms on your own environments and easily extend to more complex variations (e.g. TRPO, PPO).\n",
    "\n",
    "In this notebook you will learn how to:\n",
    "- Use open source reinforcement learning RL environments\n",
    "- Set up the training pipelines for RL\n",
    "- Testing different environments and reward engineering\n",
    "- Implementing Policy Gradient method REINFORCE and its variation with a baseline function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the environment\n",
    "\n",
    "For this assignment we will work with the gym-minigrid package, an efficient implementation of a 2D gridworld environment that will allow you to quickly train policies and verify your implementation of Policy Gradients.\n",
    "\n",
    "You can find the package documentation here: https://github.com/maximecb/gym-minigrid (It should already be installed through requirements.txt)\n",
    "\n",
    "When starting a Deep RL project it is always a good idea to rely on widely used environment implementations, as those will be well tested and results on them are easily reproducible by others. In this first part of the assignment you will learn how to set-up an opensource environment and integrate it into your project.\n",
    "\n",
    "If it is not automatically installed through requirements.txt file, you can follow the installation instructions from the Github repo of gym-minigrid. To verify that your installation worked we included the script `test_env_manual.py` that lets you play test environments with your keyboard (use only UP, LEFT and RIGHT keys). Please continue after you verified that you are able to complete one level with keyboard inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering images from the environment (3 pts)\n",
    "\n",
    "To debug the progress of our RL loop we will need to render the state of the environment.\n",
    "Here we show how to load an instance of the environment type `MiniGrid-Empty-8x8-v0`, reset it, selecting an action, taking an action step in the environment, and finally rendering the current state.\n",
    "\n",
    "\n",
    "Follow this carefully for your Todo task in the next block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_minigrid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('MiniGrid-Empty-5x5-v0')\n",
    "env.reset()\n",
    "before_img = env.render('rgb_array')\n",
    "action = env.actions.forward\n",
    "obs, reward, done, info = env.step(action)\n",
    "after_img = env.render('rgb_array')\n",
    "\n",
    "plt.imshow(np.concatenate([before_img, after_img], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with your environment (2 pts)\n",
    "\n",
    "Following the above code, Create an instance of the environment type `MiniGrid-Empty-8x8-v0`, reset, take an action step *right* and get the outputs of the environment's `step()` function. Finally, render the new state of the environment to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################# TODO: YOUR CODE BELOW #############################\n",
    "#################  Each todo here is 1 line of code  ################\n",
    "\n",
    "# 1. Make a new environment MiniGrid-Empty-8x8-v0\n",
    "env = None\n",
    "\n",
    "# 2. Reset the environment\n",
    "pass\n",
    "\n",
    "# 3. Select the action right\n",
    "action = None\n",
    "\n",
    "# 4. Take a step in the environment and store it in appropriate variables\n",
    "pass\n",
    "\n",
    "# 5. Render the current state of the environment\n",
    "img = None\n",
    "################# YOUR CODE ENDS HERE ###############################\n",
    "\n",
    "print('Observation:', obs)\n",
    "print('Reward:', reward)\n",
    "print('Done:', done)\n",
    "print('Info:', info)\n",
    "print('Image shape:', img.shape)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying an existing environment (2 pts)\n",
    "\n",
    "As you can see the environment's observation output also contains a rendered image as state representation (`obs['image']`). However, for this assignment we want to train out deep RL agent from **non-image state** input (for faster convergence).\n",
    "\n",
    "We will create a slightly altered version of the [FullyObsWrapper](https://github.com/maximecb/gym-minigrid/blob/master/gym_minigrid/wrappers.py#L223) from the repository who's observation function returns a flat vector representing the full grid.\n",
    "\n",
    "TODO: Please complete the wrapper class below and verify that a wrapped environment returns a flat vector observation of the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym_minigrid.minigrid import OBJECT_TO_IDX, COLOR_TO_IDX\n",
    "\n",
    "max_env_steps = 50\n",
    "\n",
    "class FlatObsWrapper(gym.core.ObservationWrapper):\n",
    "    \"\"\"Fully observable gridworld returning a flat grid encoding.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "        # Since the outer walls are always present, we remove left, right, top, bottom walls\n",
    "        # from the observation space of the agent. There are 3 channels, but for simplicity\n",
    "        # in this assignment, we will deal with flattened version of state.\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=((self.env.width-2) * (self.env.height-2) * 3,),  # number of cells\n",
    "            dtype='uint8'\n",
    "        )\n",
    "        self.unwrapped.max_steps = max_env_steps\n",
    "\n",
    "    def observation(self, obs):\n",
    "        # this method is called in the step() function to get the observation\n",
    "        # we provide code that gets the grid state and places the agent in it\n",
    "        env = self.unwrapped\n",
    "        full_grid = env.grid.encode()\n",
    "        full_grid[env.agent_pos[0]][env.agent_pos[1]] = np.array([\n",
    "            OBJECT_TO_IDX['agent'],\n",
    "            COLOR_TO_IDX['red'],\n",
    "            env.agent_dir\n",
    "        ])\n",
    "        full_grid = full_grid[1:-1, 1:-1]   # remove outer walls of the environment (for efficiency)\n",
    "        \n",
    "        ############################### TODO: YOUR CODE BELOW ##############################\n",
    "        ### Your code should return a flattened version of the grid.                     ###\n",
    "        ####################################################################################\n",
    "        flattened_grid = None\n",
    "        ################################# END OF YOUR CODE #################################\n",
    "        return flattened_grid\n",
    "    \n",
    "    def render(self, *args, **kwargs):\n",
    "        \"\"\"This removes the default visualization of the partially observable field of view.\"\"\"\n",
    "        kwargs['highlight'] = False\n",
    "        return self.unwrapped.render(*args, **kwargs)\n",
    "\n",
    "\n",
    "################# TODO: YOUR CODE BELOW #############################\n",
    "### 1. Create environment\n",
    "### 2. Wrap the environment using FlatObsWrapper class\n",
    "### 3. Reset the new environment\n",
    "### 4. Step with some action in the environment\n",
    "### 5. Render the grid\n",
    "#####################################################################\n",
    "obs, reward, done, info, img = None, None, None, None, None\n",
    "\n",
    "################# YOUR CODE ENDS HERE ###############################\n",
    "\n",
    "print('Observation:', obs, ', Observation Shape: ', obs.shape)\n",
    "print('Reward:', reward)\n",
    "print('Done:', done)\n",
    "print('Info:', info)\n",
    "print('Image shape:', img.shape)\n",
    "plt.close()\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Videos\n",
    "\n",
    "As the final step in environment setup we provide a helper function to log videos of policy rollouts in the environment (see utils.py). Please check whether you can render the rollout of a random policy in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "from utils import show_video\n",
    "\n",
    "# Monitor is a gym wrapper, which helps easy rendering of videos of the wrapped environment.\n",
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env\n",
    "\n",
    "def gen_wrapped_env(env_name):\n",
    "    return wrap_env(FlatObsWrapper(gym.make(env_name)))\n",
    "\n",
    "\n",
    "# Random agent - we only use it in this cell for demonstration\n",
    "class RandPolicy:\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "        \n",
    "    def act(self, *unused_args):\n",
    "        return self.action_space.sample(), None\n",
    "\n",
    "# This function plots videos of rollouts (episodes) of a given policy and environment\n",
    "def log_policy_rollout(policy, env_name, pytorch_policy=False):\n",
    "    # Create environment with flat observation\n",
    "    env = gen_wrapped_env(env_name)\n",
    "\n",
    "    # Initialize environment\n",
    "    observation = env.reset()\n",
    "\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "\n",
    "    # Run until done == True\n",
    "    while not done:\n",
    "      # Take a step\n",
    "        if pytorch_policy: \n",
    "            observation = torch.tensor(observation, dtype=torch.float32)\n",
    "            action = policy.act(observation)[0].data.cpu().numpy()\n",
    "        else:\n",
    "            action = policy.act(observation)[0]\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "\n",
    "    print('Total reward:', episode_reward)\n",
    "    print('Total length:', episode_length)\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    show_video()\n",
    "\n",
    "# Test that the logging function is working\n",
    "test_env_name = 'MiniGrid-Empty-8x8-v0'\n",
    "rand_policy = RandPolicy(FlatObsWrapper(gym.make(test_env_name)).action_space)\n",
    "log_policy_rollout(rand_policy, test_env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constructing the Rollout Buffer\n",
    "\n",
    "Now that we can create an environment instance we can start implementing the components of the policy training framework. On a very high level, the policy gradient training loop looks like this:\n",
    "\n",
    "1. Collect rollouts by running the current policy, store all trajectories.\n",
    "2. Compute policy gradient using stored rollouts. Update the policy.\n",
    "3. Repeat.\n",
    "\n",
    "First, we will look into the class that stores all rollouts collected with the current policy. For this assignment we provide the class `RolloutStorage` that implements a simple storage structure for storing the information of arbitrarily many policy rollouts.\n",
    "\n",
    "Please look at the cell below and familiarize yourself with the interface the class provides. In particular focus on the functions:\n",
    "\n",
    "- `insert(...)`: adds a new trajectory step to the rollout storage\n",
    "- `compute_returns(...)`: computes the future discounted returns for all rollout time steps after rollout collection is completed\n",
    "- `batch_sampler(...)`: returns an iterator over rollout batches, which we will use for the policy gradient update\n",
    "\n",
    "You can ignore the function `estimate_returns(...)` for now, we will make use of it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "class RolloutStorage():\n",
    "    def __init__(self, rollout_size, obs_size):\n",
    "        '''\n",
    "        rollout_size: The size of the rollout buffer\n",
    "        obs_size: The dimension of the observation vector\n",
    "        '''\n",
    "        self.rollout_size = rollout_size\n",
    "        self.obs_size = obs_size\n",
    "        self.reset()\n",
    "        \n",
    "    def insert(self, step, done, action, log_prob, reward, obs):\n",
    "        '''\n",
    "        Inserting the transition at the current step number in the environment.\n",
    "        '''\n",
    "        self.done[step].copy_(done)\n",
    "        self.actions[step].copy_(action)\n",
    "        self.log_probs[step].copy_(log_prob)\n",
    "        self.rewards[step].copy_(reward)\n",
    "        self.obs[step].copy_(obs)\n",
    "        \n",
    "    def reset(self):\n",
    "        '''\n",
    "        Initialize all storage buffers with zeros.\n",
    "        '''\n",
    "        self.done = torch.zeros(self.rollout_size, 1)\n",
    "        self.returns = torch.zeros(self.rollout_size+1, 1, requires_grad=False)\n",
    "        self.actions = torch.zeros(self.rollout_size, 1, dtype=torch.int64)  # Assuming Discrete Action Space\n",
    "        self.log_probs = torch.zeros(self.rollout_size, 1)\n",
    "        self.rewards = torch.zeros(self.rollout_size, 1)\n",
    "        self.obs = torch.zeros(self.rollout_size, self.obs_size)\n",
    "        \n",
    "    def compute_returns(self, gamma):\n",
    "        '''\n",
    "        Compute cumulative discounted returns from the current state to the end of the episode.\n",
    "        '''\n",
    "        self.last_done = (self.done == 1).nonzero().max()  # Find point of last episode's end in buffer.\n",
    "        self.returns[self.last_done+1] = 0.  # Initialize the return at the end of last episode to be 0.\n",
    "\n",
    "        # Accumulate discounted returns using dynamic programming.\n",
    "        # Cumulative return = reward from current step + discounted future reward until end of episode.\n",
    "        for step in reversed(range(self.last_done+1)):\n",
    "            self.returns[step] = self.rewards[step] + \\\n",
    "                                self.returns[step + 1] * gamma * (1 - self.done[step])\n",
    "        \n",
    "    def batch_sampler(self, batch_size, get_old_log_probs=False):\n",
    "        '''\n",
    "        Create a batch sampler of indices. Return actions, returns, observation for training.\n",
    "        get_old_log_probs: This is required for PPO to recall the log_prob of the action w.r.t.\n",
    "                           the policy that generated this transition.\n",
    "        '''\n",
    "        sampler = BatchSampler(\n",
    "            SubsetRandomSampler(range(self.last_done)),\n",
    "            batch_size,\n",
    "            drop_last=True)\n",
    "        for indices in sampler:\n",
    "            if get_old_log_probs:\n",
    "                yield self.actions[indices], self.returns[indices], self.obs[indices], self.log_probs[indices]\n",
    "            else:\n",
    "                yield self.actions[indices], self.returns[indices], self.obs[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Constructing the Policy Network (10 pts)\n",
    "\n",
    "Now that we can store rollouts we need a policy to collect them. In the following you will complete the provided base code for the policy class. The policy is instantiated as a small neural network with simple fully-connected layers, the `ActorNetwork`. \n",
    "\n",
    "Start your implementation by constructing the network in `ActorNetwork`'s constructor using the `pytorch` modules `nn.Sequential` and `nn.Linear`. \n",
    "\n",
    "After the network is constructed we can complete the `Policy` class that uses the actor to implement the `act(...)` and `update(...)` functions. Please follow the instructions in the TODO blocks for completing the implementations. The usual gradient function for the policy update takes the following form:\n",
    "\n",
    "$$ \\nabla J(\\theta) = \\mathbb{E}_{\\pi}\\big[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a, s) \\; R_t(s) \\big] $$\n",
    "\n",
    "Here, $\\theta$ are the parameters of the policy network $\\pi_{\\theta}$ and $R_t(s)$ is the observed future discounted return from state $s$ onwards, which should be **maximized**.\n",
    "\n",
    "**Entropy Loss:** In order to encourage exploration, it is a common practice to add a weighted entropy-loss component that maximizes the entropy ($\\mathcal{H}$) of the policy distribution, so it takes diverse actions. So the joint objective becomes:\n",
    "\n",
    "$$ \\nabla J(\\theta) = \\mathbb{E}_{\\pi}\\big[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a, s) \\; R_t(s) \\big] + \\nabla_{\\theta}\\mathcal{H}\\big[\\pi_\\theta(a, s)\\big]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from utils import count_model_params\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        ############################## TODO: YOUR CODE BELOW ###############################\n",
    "        ### 1. Build the Actor network as a torch.nn.Sequential module                   ###\n",
    "        ###    with the following layers:                                                ###\n",
    "        ###        (1) a Linear layer mapping from input dimension to hidden dimension   ###\n",
    "        ###        (2) a Tanh non-linearity                                              ###\n",
    "        ###        (3) a Linear layer mapping from hidden dimension to hidden dimension  ###\n",
    "        ###        (4) a Tanh non-linearity                                              ###\n",
    "        ###        (5) a Linear layer mapping from hidden dimension to number of actions ###\n",
    "        ### HINT: We do not need an activation on the output, because the actor is       ###\n",
    "        ###       predicting logits for categorical distribution.                        ###\n",
    "        ####################################################################################\n",
    "        self.fc = None\n",
    "        ################################# END OF YOUR CODE #################################\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.fc(state)\n",
    "        return x\n",
    "\n",
    "class Policy():\n",
    "    '''\n",
    "    Policy Class used for acting in the environment and updating the policy network.\n",
    "    '''\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim, learning_rate,\n",
    "                 batch_size, policy_epochs, entropy_coef=0.001):\n",
    "        self.actor = ActorNetwork(num_inputs, num_actions, hidden_dim)\n",
    "        self.optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)\n",
    "        self.batch_size = batch_size\n",
    "        self.policy_epochs = policy_epochs\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "    def act(self, state):\n",
    "        ############################## TODO: YOUR CODE BELOW ###############################\n",
    "        ### 1. Run the actor network on the current state to get the action logits       ###\n",
    "        ### 2. Build a Categorical(...) instance from the logits                         ###\n",
    "        ### 3. Sample an action using the built-in sample() function of distribution.    ###\n",
    "        ### Documentation of Categorical:                                                ###\n",
    "        ### https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical\n",
    "        ####################################################################################\n",
    "        logits = None\n",
    "        dist = None\n",
    "        action = None\n",
    "        ################################# END OF YOUR CODE #################################\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action, log_prob\n",
    "    \n",
    "    def evaluate_actions(self, state, action):\n",
    "        '''\n",
    "        Evaluate the log probability of an action under the policy's output\n",
    "        distribution for a given state.\n",
    "        \n",
    "        state -> tensor: [batch_size, obs_size]\n",
    "        action -> tensor: [batch_size, 1]\n",
    "        '''\n",
    "        ############################## TODO: YOUR CODE BELOW ###############################\n",
    "        ### This function is used for policy update to evaluate log_prob and entropy of  ###\n",
    "        ### actor network.                                                               ###\n",
    "        ### TODO: \n",
    "        ### 1. Compute logits and distribution for the given state (just like above).\n",
    "        ### 2. Compute log probability of the given action under this distribution.\n",
    "        ###    Hint: Input to the distribution should be in the shape [batch_size].\n",
    "        ###          You may find `action.squeeze(...)` helpful.\n",
    "        ### 3. Compute the entropy of the distribution.\n",
    "        ####################################################################################\n",
    "        logits = None\n",
    "        dist = None\n",
    "        log_prob = None\n",
    "        entropy = None\n",
    "        ################################# END OF YOUR CODE #################################\n",
    "        return log_prob.view(-1, 1), entropy.view(-1, 1)\n",
    "    \n",
    "    def update(self, rollouts):\n",
    "        '''\n",
    "        Performing policy gradient update with maximum entropy regularization\n",
    "        \n",
    "        rollouts -> The storage buffer\n",
    "        self.policy_epochs -> Number of times we train over the storage buffer\n",
    "        '''\n",
    "        for epoch in range(self.policy_epochs):\n",
    "            data = rollouts.batch_sampler(self.batch_size)\n",
    "            \n",
    "            for sample in data:\n",
    "                actions_batch, returns_batch, obs_batch = sample\n",
    "                \n",
    "                # Compute Log probabilities and entropy for each sampled (state, action)\n",
    "                log_probs_batch, entropy_batch = self.evaluate_actions(obs_batch, actions_batch)\n",
    "    \n",
    "                ############################## TODO: YOUR CODE BELOW ###############################\n",
    "                ### 4. Compute the mean loss for the policy update using action log-             ###\n",
    "                ###     probabilities and policy returns                                         ###\n",
    "                ### 5. Compute the mean entropy for the policy update                            ###\n",
    "                ###    *HINT*: PyTorch optimizer is used to minimize by default.                 ###\n",
    "                ###     The trick to maximize a quantity is to negate its corresponding loss.    ###\n",
    "                ####################################################################################\n",
    "                policy_loss = None\n",
    "                entropy_loss = None\n",
    "                ################################# END OF YOUR CODE #################################\n",
    "                \n",
    "                loss = policy_loss + self.entropy_coef * entropy_loss\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward(retain_graph=False)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "    @property\n",
    "    def num_params(self):\n",
    "        return count_model_params(self.actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting up the Training Loop (6 pts)\n",
    "\n",
    "Now that all classes are implemented we can combine them in the training loop and check whether everything is working!\n",
    "\n",
    "We provide a scaffolding for the `train(...)` function below. It contains some code for logging of results and has TODO blocks where you should implement the key components of the training loop. Please go through the blocks one-by-one and complete the implementation.\n",
    "\n",
    "We will use this `train(...)` function for all experiments below. You can test it in the following code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from utils import AverageMeter, plot_learning_curve\n",
    "import time\n",
    "\n",
    "def train(env, rollouts, policy, params):\n",
    "    rollout_time, update_time = AverageMeter(), AverageMeter()  # Loggers\n",
    "    rewards, success_rate = [], []\n",
    "\n",
    "    print(\"Training model with {} parameters...\".format(policy.num_params))\n",
    "\n",
    "    '''\n",
    "    Training Loop\n",
    "    '''\n",
    "    for j in range(params.num_updates):\n",
    "        ## Initialization\n",
    "        avg_eps_reward, avg_success_rate = AverageMeter(), AverageMeter()\n",
    "        done = False\n",
    "        prev_obs = env.reset()\n",
    "        prev_obs = torch.tensor(prev_obs, dtype=torch.float32)\n",
    "        eps_reward = 0.\n",
    "        start_time = time.time()\n",
    "        \n",
    "        ## Collect rollouts\n",
    "        for step in range(rollouts.rollout_size):\n",
    "            if done:\n",
    "                # Store episode statistics\n",
    "                avg_eps_reward.update(eps_reward)\n",
    "                if 'success' in info: \n",
    "                    avg_success_rate.update(int(info['success']))\n",
    "\n",
    "                # Reset Environment\n",
    "                obs = env.reset()\n",
    "                obs = torch.tensor(obs, dtype=torch.float32)\n",
    "                eps_reward = 0.\n",
    "            else:\n",
    "                obs = prev_obs\n",
    "\n",
    "            ############################## TODO: YOUR CODE BELOW ###############################\n",
    "            ### 1. Call the policy to get the action for the current observation,            ###\n",
    "            ### 2. Take one step in the environment (using the policy's action)              ###\n",
    "            ####################################################################################\n",
    "            action, log_prob = None, None\n",
    "            obs, reward, done, info = None, None, None, None\n",
    "            ################################# END OF YOUR CODE #################################\n",
    "\n",
    "            \n",
    "            ############################## TODO: YOUR CODE BELOW ###############################\n",
    "            ### 3. Insert the sample <done, action, log_prob, reward, prev_obs> in the       ###\n",
    "            ###    rollout storage. (requires just 1 line)                                   ###\n",
    "            ### HINT:                                                                        ###\n",
    "            ### - 'done' and 'reward' need to be converted to float32 tensors first          ###\n",
    "            ### - Remember we are storing the previous observation because                   ###\n",
    "            ###   that's what decided the policy's action                                    ###\n",
    "            ####################################################################################\n",
    "            pass\n",
    "            ################################# END OF YOUR CODE #################################\n",
    "            \n",
    "            prev_obs = torch.tensor(obs, dtype=torch.float32)\n",
    "            eps_reward += reward\n",
    "        \n",
    "        ############################## TODO: YOUR CODE BELOW ###############################\n",
    "        ### 4. Use the rollout buffer's function to compute the returns for all          ###\n",
    "        ###    stored rollout steps. Discount factor is given in 'params'                ###\n",
    "        ### HINT: This requires just 1 line of code.                                     ###\n",
    "        ####################################################################################\n",
    "        pass\n",
    "        ################################# END OF YOUR CODE #################################\n",
    "        \n",
    "        rollout_done_time = time.time()\n",
    "\n",
    "        ############################## TODO: YOUR CODE BELOW ###############################\n",
    "        ### 5. Call the policy's update function using the collected rollouts            ###\n",
    "        ####################################################################################\n",
    "        pass\n",
    "        ################################# END OF YOUR CODE #################################\n",
    "\n",
    "        update_done_time = time.time()\n",
    "        rollouts.reset()\n",
    "\n",
    "        ## log metrics\n",
    "        rewards.append(avg_eps_reward.avg)\n",
    "        if avg_success_rate.count > 0:\n",
    "            success_rate.append(avg_success_rate.avg)\n",
    "        rollout_time.update(rollout_done_time - start_time)\n",
    "        update_time.update(update_done_time - rollout_done_time)\n",
    "        print('it {}: avgR: {:.3f} -- rollout_time: {:.3f}sec -- update_time: {:.3f}sec'.format(j, avg_eps_reward.avg, \n",
    "                                                                                                rollout_time.avg, \n",
    "                                                                                                update_time.avg))\n",
    "        if j % params.plotting_iters == 0 and j != 0:\n",
    "            plot_learning_curve(rewards, success_rate, params.num_updates)\n",
    "            log_policy_rollout(policy, params.env_name, pytorch_policy=True)\n",
    "    clear_output()   # this removes all training outputs to keep the notebook clean, DON'T REMOVE THIS LINE!\n",
    "    return rewards, success_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate:** We provide a function to instantiate the environment, rollout buffer and policy class given their parameters.\n",
    "\n",
    "**Note about setting seeds**: RL training is notoriously unstable. Especially in the beginning of training random chance can have a large influence on the training progress, which is why RL practitioners should always report results averaged across multiple random seeds. We therefore fix the random seed at the beginning of training, to ensure reproducibility. However, if you observe that your algorithm is not training it can be beneficial to try another run with a different random seed to check whether the last run was just \"unlucky\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ParamDict\n",
    "import copy\n",
    "\n",
    "def instantiate(params_in, nonwrapped_env=None, seed=123):\n",
    "    '''\n",
    "    SETTING SEED: it is good practice to set seeds when running experiments to keep results comparable\n",
    "    '''\n",
    "    import numpy as np\n",
    "    np.random.seed(seed)\n",
    "    import torch\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    params = copy.deepcopy(params_in)\n",
    "\n",
    "    '''\n",
    "    1. Instantiate the environment\n",
    "    # If an environment is given as input to this function, we directly use that.\n",
    "    # Else, we use the environment specified in `params`.\n",
    "    '''\n",
    "    if nonwrapped_env is None:\n",
    "        nonwrapped_env = gym.make(params.env_name)\n",
    "    env = FlatObsWrapper(nonwrapped_env)\n",
    "    \n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.n\n",
    "    env.seed(seed)   # Required for reproducibility in stochastic environments.\n",
    "\n",
    "    '''\n",
    "    2. Instantiate Rollout Buffer and Policy\n",
    "    '''\n",
    "    rollouts = RolloutStorage(params.rollout_size, obs_size)\n",
    "    policy_class = params.policy_params.pop('policy_class')    \n",
    "    policy = policy_class(obs_size, num_actions, **params.policy_params)\n",
    "    \n",
    "    return env, rollouts, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting hyperparameters\n",
    "\n",
    "We start by defining a default set of hyperparameters, you can play with different values for those later.  \n",
    "However, to enable fair comparison, **YOU NEED TO SUBMIT RESULTS WITH THE DEFAULT HYPERPARAMETERS**!\n",
    "\n",
    "The hyperparameters below also define the environment that we will start our experiments with: it is a **tiny maze** without obstacles that provides a good test-bed for verifying our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "policy_params = ParamDict(\n",
    "    policy_class = Policy,    # Policy class to use (replaced later)     \n",
    "    hidden_dim = 32,          # dimension of the hidden state in actor network\n",
    "    learning_rate = 1e-3,     # learning rate of policy update\n",
    "    batch_size = 1024,        # batch size for policy update\n",
    "    policy_epochs = 4,        # number of epochs per policy update\n",
    "    entropy_coef = 0.001,     # hyperparameter to vary the contribution of entropy loss\n",
    ")\n",
    "params = ParamDict(\n",
    "    policy_params = policy_params,\n",
    "    rollout_size = 2050,      # number of collected rollout steps per policy update\n",
    "    num_updates = 50,         # number of training policy iterations\n",
    "    discount = 0.99,          # discount factor\n",
    "    plotting_iters = 10,      # interval for logging graphs and policy rollouts\n",
    "    env_name = 'MiniGrid-Empty-5x5-v0',  # we are using a tiny environment here for testing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training (4 pts)\n",
    "Using the parameters defined above, we build our environment, rollouts buffer and policy. Next, we use these to call the training loop. During training, the provided code will print out an updated learning curve and qualitative policy rollouts every couple of training iterations so that you can monitor progress.\n",
    "\n",
    "If everything is set up correctly the policy should should start learning quickly, resulting in increasing average reward per episode.\n",
    "\n",
    "**Expected time per rollout-update iteration: about 1-5 seconds** (for all the experiments)\n",
    "\n",
    "The code will automatically erase all training outputs at the end of training to clean up the notebook and keeps its length manageable for grading. You can execute the next cell to evaluate your trained policy and get the full training curve as well as a number of qualitative policy rollouts. **PLEASE MAKE SURE THAT YOUR FINAL SUBMISSION DOES NOT INLCUDE THE DETAILED TRAINING OUTPUTS, WE WILL DEDUCT POINTS OTHERWISE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, rollouts, policy = instantiate(params)\n",
    "rewards, success_rate = train(env, rollouts, policy, params)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "To evaluate the training run please use the next cell to plot the final learning curve and a few sample rollouts from the trained policy. (these results **should** be in your final submission)\n",
    "\n",
    "You can re-run the following block multiple times to render more evaluation videos.\n",
    "\n",
    "**HINT**: After ~50 training iterations your policy should reach a mean return >0.5 and reach the goal in some of the qualitative samples we plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final reward + policy plotting for easier evaluation\n",
    "plot_learning_curve(rewards, success_rate, params.num_updates)\n",
    "for _ in range(3):\n",
    "    log_policy_rollout(policy, params.env_name, pytorch_policy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing on more complex environments (2 pts)\n",
    "\n",
    "Now that we have seen that we can solve a very simple environment with vanilla policy gradients, let's try harder ones and see how the algorithm behaves!\n",
    "\n",
    "We start with a **bigger gridworld**. This makes the problem *sparser*, i.e. the algorithm sees less rewards early on in training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "policy_params = ParamDict(\n",
    "    policy_class = Policy,    # Policy class to use (replaced later)\n",
    "    hidden_dim = 32,          # dimension of the hidden state in actor network\n",
    "    learning_rate = 1e-3,     # learning rate of policy update\n",
    "    batch_size = 1024,        # batch size for policy update\n",
    "    policy_epochs = 4,        # number of epochs per policy update\n",
    "    entropy_coef = 0.001,     # hyperparameter to vary the contribution of entropy loss\n",
    ")\n",
    "params = ParamDict(\n",
    "    policy_params = policy_params,\n",
    "    rollout_size = 2050,      # number of collected rollout steps per policy update\n",
    "    num_updates = 50,         # number of training policy iterations\n",
    "    discount = 0.99,          # discount factor\n",
    "    plotting_iters = 10,      # interval for logging graphs and policy rollouts\n",
    "    env_name = 'MiniGrid-Empty-8x8-v0',  # we are using a tiny environment here for testing\n",
    ")\n",
    "\n",
    "\n",
    "env, rollouts, policy = instantiate(params)\n",
    "rewards, success_rate = train(env, rollouts, policy, params)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - bigger gridworld\n",
    "Now again plot the final training curve as well as some policy rollouts.\n",
    "\n",
    "**HINT**: The average episode reward should also reach ~0.5 after ~50 iterations. If you can't achieve this result try rerunning your training, randomness can play a big role in the beginning of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final reward + policy plotting for easier evaluation\n",
    "plot_learning_curve(rewards, success_rate, params.num_updates)\n",
    "for _ in range(3):\n",
    "    log_policy_rollout(policy, params.env_name, pytorch_policy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Question (4 pts):\n",
    "\n",
    "(1) Please compare the training curves for the two grids of different size. (2-line answer)\n",
    "\n",
    "**Ans:**\n",
    "___\n",
    "\n",
    "(2) Explain why the bigger maze tends to learn slower in the beginning of training. (2-line answer)\n",
    "\n",
    "**Ans:**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploring the effect of Reward Design\n",
    "\n",
    "Next we will test our algorithm on an even more challenging environment that includes obstacles in the form of walls and a bottleneck doorway the agent needs to pass through to reach the goal.\n",
    "\n",
    "![](./grid_img.png)\n",
    "\n",
    "In this environment we will investigate the effect of different reward choices on the training dynamics and later how we can improve the vanilla REINFORCE algorithm we implemented above.\n",
    "\n",
    "Rewards are central to RL as they provide guidance to the policy about the desired agent behavior. There are many different ways to specify rewards and each of them has their own trade-offs: from **easy to specify but hard to learn** all the way to **challenging to specify, but easy learning**. Let's explore the trade-offs we can make!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation with multiple seeds (2 pts)\n",
    "\n",
    "Before we move on to experimentation, an important aspect in reinforcement learning is to understand the impact of seed variability on experiments. Different seeds can affect training and exploration a lot, therefore it is common practice in RL research to run multiple seeds and plot the mean $\\pm$ std of the results. This problem becomes more adverse on more complex environments, as randomness can have bigger influence on the results of training.\n",
    "\n",
    "Therefore, for the following experiments, we will follow this practice and ask you to run **three random seeds** for the final submission results (Note that, in practice the more seeds the better).\n",
    "\n",
    "You are free to run less seeds to debug your code, however we do **REQUIRE YOU TO SUBMIT RESULTS FOR ALL EXPERIMENTS BELOW WITH THREE RANDOM SEEDS**, otherwise we **WILL DEDUCT POINTS**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seeds = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the previous settings (bigger gridworld environment) with 3 seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, success_rates = [], []\n",
    "for i in range(n_seeds):\n",
    "    print(\"Start training run {}!\".format(i))\n",
    "    env, rollouts, policy = instantiate(params, seed=i)\n",
    "    r, sr = train(env, rollouts, policy, params)\n",
    "    rewards.append(r); success_rates.append(sr)\n",
    "print('All training runs completed!')\n",
    "\n",
    "plot_learning_curve(rewards, success_rates, params.num_updates, plot_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Reward Environment (2 pts)\n",
    "We start with the easiest to define version of rewards: sparse rewards. The idea is that we just define a very sparse signal, usually an indicator that some goal was achieved. This is in most cases pretty convenient, e.g. we can just detect whether our agent reached the goal and provide a positive reward if that was the case. \n",
    "\n",
    "The limitations of this formulation become quickly apparent when we think of more complex tasks. For example if an autonomous car would only receive a reward once it drove us to our goal location it would not get a lot of guidance on the way. As a rule of thumb: sparse rewards are easy to specify but usually hard to learn from.\n",
    "\n",
    "We explore sparse rewards in our test environment by providing a reward of one when the agent reaches the target (minus a small penalty depending on how long it took the agent to get there) and zero reward otherwise. Run the training code on this new environment and see how the algorithm behaves. Since these environments are harder, we train it for longer (see *num_updates*).\n",
    "\n",
    "**HINT**: If you want to explore the sparse reward environment or any of the following environments you can use the same `test_env_manual.py` script you used in the beginning to navigate using your keyboard. This will show you what rewards the agent receives in the different environments. Just make sure to uncomment the respective environment construction in the beginning of the `test_env_manual.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "from minigrid_utils import DeterministicCrossingEnv\n",
    "import warnings\n",
    "\n",
    "def gen_wrapped_env(env_name):\n",
    "    if env_name is 'det_sparse':\n",
    "        warnings.filterwarnings(\"ignore\")   # suppress warning when plotting\n",
    "        return wrap_env(FlatObsWrapper(DeterministicCrossingEnv()))\n",
    "    return wrap_env(FlatObsWrapper(gym.make(env_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "policy_params = ParamDict(\n",
    "    policy_class = Policy,    # Policy class to use (replaced later)\n",
    "    hidden_dim = 32,          # dimension of the hidden state in actor network\n",
    "    learning_rate = 1e-3,     # learning rate of policy update\n",
    "    batch_size = 1024,        # batch size for policy update\n",
    "    policy_epochs = 4,        # number of epochs per policy update\n",
    "    entropy_coef = 0.001,     # hyperparameter to vary the contribution of entropy loss\n",
    ")\n",
    "params = ParamDict(\n",
    "    policy_params = policy_params,\n",
    "    rollout_size = 2050,      # number of collected rollout steps per policy update\n",
    "    num_updates = 150,         # number of training policy iterations\n",
    "    discount = 0.99,          # discount factor\n",
    "    plotting_iters = 10,      # interval for logging graphs and policy rollouts\n",
    "    env_name = 'det_sparse',\n",
    ")\n",
    "\n",
    "rewards_sparse, success_rates_sparse = [], []\n",
    "for i in range(n_seeds):\n",
    "    print(\"Start training run {}!\".format(i))\n",
    "    env, rollouts, policy = instantiate(params, nonwrapped_env=DeterministicCrossingEnv(), seed=i)\n",
    "    r, sr = train(env, rollouts, policy, params)\n",
    "    rewards_sparse.append(r); success_rates_sparse.append(sr)\n",
    "print('All training runs completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(rewards_sparse, success_rates_sparse, params.num_updates, plot_std=True)\n",
    "for _ in range(3):\n",
    "    log_policy_rollout(policy, params.env_name, pytorch_policy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Densify\" Reward\n",
    "\n",
    "Sparse rewards are easy to specify but only provide very little feedback to the algorithm for what constitutes desirable behavior. As you have seen in the previous examples, this makes it very challenging to effectively learn a good policy. In practice, we therefore usually resort to constructing a more dense reward that gives the policy more \"guidance\" during learning. How to make the reward more dense is highly problem-dependent. \n",
    "\n",
    "Below we will test two custom environments that provide different options for a more dense reward on the obstacle-maze task and investigate whether this helps training the policy.\n",
    "\n",
    "First we will overwrite the `gen_wrapped_env(...)` method used for logging such that it can handle the new environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minigrid_utils import (DeterministicCrossingEnv, DetHardEuclidCrossingEnv,\n",
    "                            DetHardSubgoalCrossingEnv)\n",
    "import warnings\n",
    "\n",
    "def gen_wrapped_env(env_name):\n",
    "    if env_name is 'det_sparse':\n",
    "        warnings.filterwarnings(\"ignore\")   # suppress warning when plotting\n",
    "        return wrap_env(FlatObsWrapper(DetSparseCrossingEnv()))\n",
    "    if env_name is 'det_subgoal':\n",
    "        warnings.filterwarnings(\"ignore\")   # suppress warning when plotting\n",
    "        return wrap_env(FlatObsWrapper(DetHardSubgoalCrossingEnv()))\n",
    "    elif env_name is 'det_euclid':\n",
    "        warnings.filterwarnings(\"ignore\")   # suppress warning when plotting\n",
    "        return wrap_env(FlatObsWrapper(DetHardEuclidCrossingEnv()))\n",
    "    return wrap_env(FlatObsWrapper(gym.make(env_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgoal Reward (2 pts)\n",
    "\n",
    "We will first try a hybrid solution between a sparse setup where the agent receives rewards only when the goal is reached and a truly dense setup where the agent receives reward in every step. Concretely, we will add one more **subgoal** to the environment to encourage the agent to reach the bottleneck state that passes through the wall. Upon reaching the subgoal the agent will receive a positive reward, encouraging it to reach the bottleneck more often. Laying out such a **\"path of breadcrumbs\"** is a standard practice in RL and we will explore whether our agent can benefit from it.\n",
    "\n",
    "Execute the cells below to train the algorithm on the subgoal-based environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "policy_params = ParamDict(\n",
    "    policy_class = Policy,    # Policy class to use (replaced later)\n",
    "    hidden_dim = 32,          # dimension of the hidden state in actor network\n",
    "    learning_rate = 1e-3,     # learning rate of policy update\n",
    "    batch_size = 1024,        # batch size for policy update\n",
    "    policy_epochs = 4,        # number of epochs per policy update\n",
    "    entropy_coef = 0.001,     # hyperparameter to vary the contribution of entropy loss\n",
    ")\n",
    "params = ParamDict(\n",
    "    policy_params = policy_params,\n",
    "    rollout_size = 2050,      # number of collected rollout steps per policy update\n",
    "    num_updates = 150,         # number of training policy iterations\n",
    "    discount = 0.99,          # discount factor\n",
    "    plotting_iters = 10,      # interval for logging graphs and policy rollouts\n",
    "    env_name = 'det_subgoal',  # we are using a tiny environment here for testing\n",
    ")\n",
    "\n",
    "rewards_subgoal, success_rates_subgoal = [], []\n",
    "for i in range(n_seeds):\n",
    "    print(\"Start training run {}!\".format(i))\n",
    "    env, rollouts, policy = instantiate(params, nonwrapped_env=DetHardSubgoalCrossingEnv(), seed=i)\n",
    "    r, sr = train(env, rollouts, policy, params)\n",
    "    rewards_subgoal.append(r); success_rates_subgoal.append(sr)\n",
    "print('All training runs completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(rewards_subgoal, success_rates_subgoal, params.num_updates, plot_std=True)\n",
    "for _ in range(3):\n",
    "    log_policy_rollout(policy, params.env_name, pytorch_policy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Reward (distance-based) (2 pts)\n",
    "\n",
    "While adding subgoals provided some guidance signal to the agent, we will try to further \"densify\" the reward formulation. Concretely, we will explore a rather straightforward way to define a dense reward: we will use the euclidean distance $d$ between the current agent position to the goal to define a reward. \n",
    "\n",
    "At this point one could be tempted to simply use the complement $1-d$ as reward such that maximizing it would lead the agent to minimize the distance to the goal. This is a good example for why specifying dense rewards is hard! If we were to choose this simple reward formulation the agent would actually be encouraged to move next to the goal,  but then **wait next to the goal** until right before the timeout of the episode is reached as terminating early is worse than receiving a long stream of high rewards while waiting next to the goal.\n",
    "\n",
    "To prevent such \"lazyness\", we instead look at the differnce of the current distance to the goal and the previous step's distance. If this difference is negative, i.e. if we moved closer to the goal with the last action, we reward the agent. If we moved away from the goal we instead return a negative reward to the agent. If we distance does not change, i.e. if the agent stays in place, we do not provide any reward. In this way the agent is encouraged to choose actions that bring it closer to the goal while there is no benefit to waiting next to the goal (the discount factor <1 ensures that waiting is actually a bad choice).\n",
    "\n",
    "Run the following cells to train an agent on the euclidean-distance-based dense reward environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "policy_params = ParamDict(\n",
    "    policy_class = Policy,    # Policy class to use (replaced later)\n",
    "    hidden_dim = 32,          # dimension of the hidden state in actor network\n",
    "    learning_rate = 1e-3,     # learning rate of policy update\n",
    "    batch_size = 1024,        # batch size for policy update\n",
    "    policy_epochs = 4,        # number of epochs per policy update\n",
    "    entropy_coef = 0.001,     # hyperparameter to vary the contribution of entropy loss\n",
    ")\n",
    "params = ParamDict(\n",
    "    policy_params = policy_params,\n",
    "    rollout_size = 2050,      # number of collected rollout steps per policy update\n",
    "    num_updates = 150,         # number of training policy iterations\n",
    "    discount = 0.99,          # discount factor\n",
    "    plotting_iters = 10,      # interval for logging graphs and policy rollouts\n",
    "    env_name = 'det_euclid',  # we are using a tiny environment here for testing\n",
    ")\n",
    "\n",
    "rewards_dense, success_rates_dense = [], []\n",
    "for i in range(n_seeds):\n",
    "    print(\"Start training run {}!\".format(i))\n",
    "    env, rollouts, policy = instantiate(params, nonwrapped_env=DetHardEuclidCrossingEnv(), seed=i)\n",
    "    r, sr = train(env, rollouts, policy, params)\n",
    "    rewards_dense.append(r); success_rates_dense.append(sr)\n",
    "print('All training runs completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(rewards_dense, success_rates_dense, params.num_updates, plot_std=True)\n",
    "for _ in range(3):\n",
    "    log_policy_rollout(policy, params.env_name, pytorch_policy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Engineered Dense Reward (4 pts)\n",
    "\n",
    "In the previous example we saw that adding dense rewards can help speeding up training, however there were some degeneracies which discouraged the agent from learning the optimal policy. This process is typical when tackling a new task with RL: one tries sparse rewards and then keeps increasing the density of the reward until the agent is able to learn the desired behavior. Along the way the agent will discover various ways in which to exploit the different reward choices (remember the \"lazy\" behavior described above), requiring tedious changes to the reward design. This process, known as \"reward engineering\" is a big pain when developing RL solutions.\n",
    "\n",
    "In the following we will sidestep this process. In our simple obstacle maze environment it is relatively straightforward to define a reward that is likely very close to \"optimal\". We will alter the dense reward from the previous section to use the **shortest-path distance** between the agent and the goal. We do this by computing the [manhattan distance](https://www.quora.com/What-is-Manhattan-Distance) while taking the walls into account.\n",
    "\n",
    "In the following code, implement manhattan distance, and then run your custom environment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minigrid_utils import DetHardOptRewardCrossingEnv\n",
    "\n",
    "class CustomEnv(DetHardOptRewardCrossingEnv):\n",
    "    def manhattan_distance(self, a, b):\n",
    "        ############################## TODO: YOUR CODE BELOW ###############################\n",
    "        ### Compute the manhattan distance between two coordinates a and b               ###\n",
    "        ### a and b are 2D numpy arrays                                                  ###\n",
    "        ####################################################################################\n",
    "        distance = None\n",
    "        ################################# END OF YOUR CODE #################################\n",
    "        return distance\n",
    "    \n",
    "def gen_wrapped_env(env_name):\n",
    "    if env_name is 'det_sparse':\n",
    "        warnings.filterwarnings(\"ignore\")   # suppress warning when plotting\n",
    "        return wrap_env(FlatObsWrapper(DetSparseCrossingEnv()))\n",
    "    if env_name is 'det_subgoal':\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        return wrap_env(FlatObsWrapper(DetHardSubgoalCrossingEnv()))\n",
    "    elif env_name is 'det_euclid':\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        return wrap_env(FlatObsWrapper(DetHardEuclidCrossingEnv()))\n",
    "    elif env_name is 'custom_env':\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        return wrap_env(FlatObsWrapper(CustomEnv()))\n",
    "    return wrap_env(FlatObsWrapper(gym.make(env_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "policy_params = ParamDict(\n",
    "    policy_class = Policy,    # Policy class to use (replaced later)\n",
    "    hidden_dim = 32,          # dimension of the hidden state in actor network\n",
    "    learning_rate = 1e-3,     # learning rate of policy update\n",
    "    batch_size = 1024,        # batch size for policy update\n",
    "    policy_epochs = 4,        # number of epochs per policy update\n",
    "    entropy_coef = 0.001,     # hyperparameter to vary the contribution of entropy loss\n",
    ")\n",
    "params = ParamDict(\n",
    "    policy_params = policy_params,\n",
    "    rollout_size = 2050,      # number of collected rollout steps per policy update\n",
    "    num_updates = 150,         # number of training policy iterations\n",
    "    discount = 0.99,          # discount factor\n",
    "    plotting_iters = 10,      # interval for logging graphs and policy rollouts\n",
    "    env_name = 'custom_env',  # we are using a tiny environment here for testing\n",
    ")\n",
    "\n",
    "rewards_custom, success_rates_custom = [], []\n",
    "for i in range(n_seeds):\n",
    "    print(\"Start training run {}!\".format(i))\n",
    "    env, rollouts, policy = instantiate(params, nonwrapped_env=CustomEnv(), seed=i)\n",
    "    r, sr = train(env, rollouts, policy, params)\n",
    "    rewards_custom.append(r); success_rates_custom.append(sr)\n",
    "print('All training runs completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(rewards_custom, success_rates_custom, params.num_updates, plot_std=True)\n",
    "for _ in range(3):\n",
    "    log_policy_rollout(policy, params.env_name, pytorch_policy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Reward Design Results (2 pt)\n",
    "In order to make comparison across reward design experiments easier, please run the following code block to make a combined code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_grid_std_learning_curves\n",
    "plots = {\n",
    "'Sparse': (rewards_sparse, success_rates_sparse),\n",
    "'Subgoal': (rewards_subgoal, success_rates_subgoal),\n",
    "'Dense': (rewards_dense, success_rates_dense),\n",
    "'Custom': (rewards_custom, success_rates_custom)}\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18.0, 14.0) # set default size of plots\n",
    "plot_grid_std_learning_curves(plots, params.num_updates)\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Questions (9 pts)\n",
    "Give **two-line** answers to the following questions:\n",
    "\n",
    "(1) Compare the performance of the different dense reward formulations. Which formulation is better and why do you think so? (3 pts)\n",
    "\n",
    "**Ans:**\n",
    "***\n",
    "(2) Mention two real world applications where it is easy to design dense rewards, and two where it is not possible? (3 pts)\n",
    "\n",
    "**Ans:**\n",
    "***\n",
    "\n",
    "\n",
    "(3) Based on the insights gain from experiments above, what is a good strategy to adopt in scenarios where good dense reward is not available? Why do you think so? (3 pts)\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Add a Baseline Network (10 pts)\n",
    "\n",
    "Since REINFORCE relies on **sampling**-based gradient estimates to update the policy, the gradients can be very noisy, which can make the training unstable and slow.\n",
    "There are many ways which attempt to reduce the variance in learning, and one such way is to use a **baseline**.\n",
    "\n",
    "The key idea of a baseline is that if we add or subtract a term $b(s_t)$ which is independent of action to the estimated returns $R_t$, then the expected value still remains the same:\n",
    "\n",
    "$$ \\nabla J(\\theta) = \\mathbb{E}_{\\pi}\\big[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a, s) \\; R_t(s) \\big] \n",
    "= \\mathbb{E}_{\\pi}\\big[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a, s) \\; [R_t(s) - b_t(s)] \\big] $$\n",
    "\n",
    "Hence, this keeps the gradient estimates unbiased (that is close to real gradients), and can help in reducing the variance if it is able to reduce the magnitude of gradients. Reduced variance can potentially lead to more stable training.\n",
    "\n",
    "A good baseline to use is the value function of a state $b_t(s) = v_t(s)$, which can be trained along with the policy. The value function acts like a **critic** to the **actor** (policy network), where the critic's objective is to estimate correct expected returns $R_t(s)$ and actor's objective is the policy gradient augmented by critic's predictions.\n",
    "\n",
    "The term $R_t(s) - v_t(s)$ is called the advantage, and it measures how well the policy is expected to do (estimated by expected rewards) as compared to the average value of a state (estimated by value function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        ############################## TODO: YOUR CODE BELOW ###############################\n",
    "        ### 1. Build the Actor network as a torch.nn.Sequential module                   ###\n",
    "        ###    with the following layers:                                                ###\n",
    "        ###        (1) a Linear layer mapping from input dimension to hidden dimension   ###\n",
    "        ###        (2) a Tanh non-linearity                                              ###\n",
    "        ###        (3) a Linear layer mapping from hidden dimension to hidden dimension  ###\n",
    "        ###        (4) a Tanh non-linearity                                              ###\n",
    "        ###        (5) a Linear layer mapping from hidden dimension to 1                 ###\n",
    "        ### HINT: We do not need an activation on the output, because the actor is       ###\n",
    "        ###       predicting a value, which can be any real number                       ###\n",
    "        ####################################################################################\n",
    "        self.fc = None\n",
    "        ################################# END OF YOUR CODE #################################\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc(state)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ACPolicy(Policy):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim, learning_rate, batch_size, policy_epochs,\n",
    "                 entropy_coef=0.001, critic_coef=0.5):\n",
    "        super().__init__(num_inputs, num_actions, hidden_dim, learning_rate, batch_size, policy_epochs, entropy_coef)\n",
    "\n",
    "        self.critic = CriticNetwork(num_inputs, hidden_dim)\n",
    "        \n",
    "        ############################## TODO: YOUR CODE BELOW ###############################\n",
    "        ### Create a common optimizer for actor and critic with the given learning rate  ###\n",
    "        ### (requires 1-line of code)                                                    ###\n",
    "        ####################################################################################\n",
    "        self.optimizer = None\n",
    "        ################################# END OF YOUR CODE #################################\n",
    "\n",
    "        self.critic_coef = critic_coef\n",
    "        \n",
    "    def update(self, rollouts): \n",
    "        for epoch in range(self.policy_epochs):\n",
    "            data = rollouts.batch_sampler(self.batch_size)\n",
    "            \n",
    "            for sample in data:\n",
    "                actions_batch, returns_batch, obs_batch = sample\n",
    "                log_probs_batch, entropy_batch = self.evaluate_actions(obs_batch, actions_batch)\n",
    "\n",
    "                value_batch = self.critic(obs_batch)\n",
    "                advantage = returns_batch - value_batch.detach()\n",
    "\n",
    "                \n",
    "                ############################## TODO: YOUR CODE BELOW ###############################\n",
    "                ### 1. Compute the mean loss for the policy update using action log-             ###\n",
    "                ###     probabilities and advantages.                                            ###\n",
    "                ### 2. Compute the mean entropy for the policy update                            ###\n",
    "                ### 3. Compute the critic loss as MSE loss between estimated value and expected  ###\n",
    "                ###     returns.                                                                 ###\n",
    "                ###    *HINT*: Carefully select the signs of each of the losses .                ###\n",
    "                ####################################################################################\n",
    "                policy_loss = None\n",
    "                entropy_loss = None\n",
    "                critic_loss = None\n",
    "                ################################# END OF YOUR CODE #################################\n",
    "                \n",
    "                loss = policy_loss + \\\n",
    "                        self.critic_coef * critic_loss + \\\n",
    "                        self.entropy_coef * entropy_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "    @property\n",
    "    def num_params(self):\n",
    "        return super().num_params + count_model_params(self.critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "policy_params = ParamDict(\n",
    "    policy_class = ACPolicy,  # Policy class to use (replaced later)\n",
    "    hidden_dim = 32,          # dimension of the hidden state in actor network\n",
    "    learning_rate = 1e-3,     # learning rate of policy update\n",
    "    batch_size = 1024,        # batch size for policy update\n",
    "    policy_epochs = 4,        # number of epochs per policy update\n",
    "    entropy_coef = 0.001,     # hyperparameter to vary the contribution of entropy loss\n",
    "    critic_coef = 0.5         # Coefficient of critic loss when weighted against actor loss\n",
    ")\n",
    "params = ParamDict(\n",
    "    policy_params = policy_params,\n",
    "    rollout_size = 2050,      # number of collected rollout steps per policy update\n",
    "    num_updates = 150,        # number of training policy iterations\n",
    "    discount = 0.99,          # discount factor\n",
    "    plotting_iters = 10,      # interval for logging graphs and policy rollouts\n",
    "    env_name = 'det_subgoal',\n",
    ")\n",
    "\n",
    "rewards_ac, success_rates_ac = [], []\n",
    "for i in range(n_seeds):\n",
    "    print(\"Start training run {}!\".format(i))\n",
    "    env, rollouts, policy = instantiate(params, nonwrapped_env=DetHardSubgoalCrossingEnv(), seed=i)\n",
    "    r, sr = train(env, rollouts, policy, params)\n",
    "    rewards_ac.append(r); success_rates_ac.append(sr)\n",
    "print('All training runs completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(rewards_ac, success_rates_ac, params.num_updates, plot_std=True)\n",
    "for _ in range(3):\n",
    "    log_policy_rollout(policy, params.env_name, pytorch_policy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions (4 pts)\n",
    "\n",
    "(1) How does the actor critic algorithm perform as compared to vanilla policy gradients?\n",
    "\n",
    "**Ans:**\n",
    "___\n",
    "\n",
    "(2) Can we make reliable comparisons with these experiments and plots, and why do you think so?\n",
    "\n",
    "**Ans:**\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Proximal Policy Optimization (PPO) (8 pts)\n",
    "\n",
    "[PPO](https://arxiv.org/abs/1707.06347) is an actor-critic algorithm which constrains the gradient updates to a policy by modifying the training objective with a clipped surrogate objective. This essentially results in a more stable optimization procedure, and hence leads to better sample efficiency and fast computation time, as compared to vanilla policy gradient methods. It is very easy to implement, and here you are supposed to modify the update function of Actor Critic Policy to incorporate these surrogate losses.\n",
    "\n",
    "$$ ratio = \\frac{\\pi^{new}(a)}{\\pi^{old}(a)} = e^{log \\pi^{new}(a) - log \\pi^{old}(a)}  $$\n",
    "$$ \\mathcal{L}_{surrogate1} = ratio * Advantage $$\n",
    "$$ \\mathcal{L}_{surrogate2} = clip (ratio, 1 - c, 1 + c) * Advantage $$\n",
    "$$ \\mathcal{L}_{policy} = - min(\\mathcal{L}_{surrogate1}, \\mathcal{L}_{surrogate2})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(ACPolicy):       \n",
    "    def update(self, rollouts): \n",
    "        self.clip_param = 0.2\n",
    "        for epoch in range(self.policy_epochs):\n",
    "            data = rollouts.batch_sampler(self.batch_size, get_old_log_probs=True)\n",
    "            \n",
    "            for sample in data:\n",
    "                actions_batch, returns_batch, obs_batch, old_log_probs_batch = sample\n",
    "                log_probs_batch, entropy_batch = self.evaluate_actions(obs_batch, actions_batch)\n",
    "                \n",
    "                value_batch = self.critic(obs_batch)\n",
    "                \n",
    "                advantage = returns_batch - value_batch.detach()\n",
    "                old_log_probs_batch = old_log_probs_batch.detach()\n",
    "\n",
    "                ############################## TODO: YOUR CODE BELOW ###############################\n",
    "                ### Compute the following terms by following the equations given above           ###\n",
    "                ### Useful functions: torch.exp(...), torch.clamp(...)\n",
    "                ### Note: self.clip_param is the c in the above equations                        ###\n",
    "                ### Compute the following terms by following the equations given above           ###\n",
    "                ####################################################################################\n",
    "                ratio = None\n",
    "                surr1 = None\n",
    "                surr2 = None\n",
    "\n",
    "                policy_loss = None\n",
    "                entropy_loss = None\n",
    "                critic_loss = None\n",
    "                ################################# END OF YOUR CODE #################################\n",
    "\n",
    "                loss = policy_loss + \\\n",
    "                        self.critic_coef * critic_loss + \\\n",
    "                        self.entropy_coef * entropy_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward(retain_graph=False)\n",
    "                self.optimizer.step()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "policy_params = ParamDict(\n",
    "    policy_class = PPO,  # Policy class to use (replaced later)\n",
    "    hidden_dim = 32,          # dimension of the hidden state in actor network\n",
    "    learning_rate = 1e-3,     # learning rate of policy update\n",
    "    batch_size = 1024,        # batch size for policy update\n",
    "    policy_epochs = 4,        # number of epochs per policy update\n",
    "    entropy_coef = 0.001,     # hyperparameter to vary the contribution of entropy loss\n",
    "    critic_coef = 0.5         # Coefficient of critic loss when weighted against actor loss\n",
    ")\n",
    "params = ParamDict(\n",
    "    policy_params = policy_params,\n",
    "    rollout_size = 2050,      # number of collected rollout steps per policy update\n",
    "    num_updates = 150,        # number of training policy iterations\n",
    "    discount = 0.99,          # discount factor\n",
    "    plotting_iters = 10,      # interval for logging graphs and policy rollouts\n",
    "    env_name = 'det_subgoal',  # we are using a tiny environment here for testing\n",
    ")\n",
    "\n",
    "rewards_ppo, success_rates_ppo = [], []\n",
    "for i in range(n_seeds):\n",
    "    print(\"Start training run {}!\".format(i))\n",
    "    env, rollouts, policy = instantiate(params, nonwrapped_env=DetHardSubgoalCrossingEnv(), seed=i)\n",
    "    r, sr = train(env, rollouts, policy, params)\n",
    "    rewards_ppo.append(r); success_rates_ppo.append(sr)\n",
    "print('All training runs completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(rewards_ppo, success_rates_ppo, params.num_updates, plot_std=True)\n",
    "for _ in range(3):\n",
    "    log_policy_rollout(policy, params.env_name, pytorch_policy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions (2 pts)\n",
    "\n",
    "How does PPO compare to the actor-critic method (i.e. with baseline and without surrogate objective) in your experimental results? (2-line answer)\n",
    "\n",
    "**Ans:**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions (3 pts)\n",
    "\n",
    "Following all the experiments in the assignment, what do you think are the bigger problems in Reinforcement Learning? e.g. exploration, sparsity, etc. Describe any 3 such problems, and how exactly you infer that from your experiments (3-line answer)\n",
    "\n",
    "**Ans:**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve your own environment! (5 pts + 7 pts)\n",
    "\n",
    "Until this point, we only dealt with static environments, where the walls do not change across episodes. For this last open-ended task, we want you to solve a more complex task of your choice - Lava or Wall.\n",
    "![](./envs.png)\n",
    "\n",
    "If the agent walks into lava, it dies and the game ends, whereas the wall environment is a bigger and randomized version of the task you have been solving.\n",
    "\n",
    "We ask you to solve a subgoal-based version of this environment, and you are free to modify any code below this block (except environment code we have given) - with your choice of algorithm and hyperparameters. Please implement the full pipeline as in examples above (taking care of env_name and environment class).\n",
    "\n",
    "**5 pts**: For successfully running the training and plotting pipeline (over >=3 seeds).\n",
    "\n",
    "**7 pts**: Achieving non-trivial accuracy - Graded relative to other students who picked your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minigrid_utils import LavaEnv, WallEnv\n",
    "    \n",
    "def gen_wrapped_env(env_name):\n",
    "    if env_name is 'lava':\n",
    "        warnings.filterwarnings(\"ignore\")   # suppress warning when plotting\n",
    "        return wrap_env(FlatObsWrapper(LavaEnv()))\n",
    "    if env_name is 'wall':\n",
    "        warnings.filterwarnings(\"ignore\")   # suppress warning when plotting\n",
    "        return wrap_env(FlatObsWrapper(WallEnv()))\n",
    "    return wrap_env(FlatObsWrapper(gym.make(env_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement and plot. We want to see the plot of your agent's performance on the environment of your choice.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
